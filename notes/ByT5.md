---
corpus_id: 235248316
title: "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models"
stars: 2
topics: [pretraining, language model, AR, NLP]
---

Incremental work: T5 -> mT5 -> ByT5. Essentially dropped the tokenizer from T5 and tuned parameters.